{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro to Deep Learning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UNWessW4qk2H"},"source":["<img src=\"https://raw.githubusercontent.com/hwhitt/DSGT-Workshop-Content/master/images/logo.png\" width=\"100\"/>\n","\n","# Workshop: Introduction to Deep Learning\n","\n","## Learning Objectives\n","- create a fully-connected neural network architecture\n","- apply neural nets to two classic ML problems: regression and classification\n","- train neural nets with stochastic gradient descent\n","- improve performance with dropout, batch normalization, and other techniques"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhDQi7daRKSU","executionInfo":{"status":"ok","timestamp":1613506327794,"user_tz":300,"elapsed":292,"user":{"displayName":"Arnesh Sahay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gigokmr-b6HDELZvM8XBx_C52WACBzX8EBXsjQbeg=s64","userId":"11472057529334045403"}},"outputId":"912d58b3-4964-4d7e-93e3-9b55e42ec006"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8FuCrPwqcZY1"},"source":["*First* create a shortcut to [this](https://drive.google.com/drive/folders/1dNGY4E4wdlSpq5IvSchaUSAkcT1-T9t8?usp=sharing) folder in your drive. Then, we'll cd into it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDLblg1Dcat0","executionInfo":{"status":"ok","timestamp":1613506364551,"user_tz":300,"elapsed":219,"user":{"displayName":"Arnesh Sahay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gigokmr-b6HDELZvM8XBx_C52WACBzX8EBXsjQbeg=s64","userId":"11472057529334045403"}},"outputId":"dd224f97-aeca-4d0b-ed49-f336b59d1848"},"source":["cd content/drive/MyDrive/Workshop\\ 5\\ -\\ DL"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1dNGY4E4wdlSpq5IvSchaUSAkcT1-T9t8/Workshop 5 - DL\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WSoHiO701ewd"},"source":["## What is Deep Learning\n","\n","Some of the most impressive advances in artificial intelligence in recent years have been in the field of deep learning. Natural language translation, image recognition, and game playing are all tasks where deep learning models have neared or even exceeded human-level performance.\n","\n","So what is deep learning? **Deep learning** is an approach to machine learning characterized by deep stacks of computations. This depth of computation is what has enabled deep learning models to disentangle the kinds of complex and hierarchical patterns found in the most challenging real-world datasets.\n","\n","Through their power and scalability **neural networks** have become the defining model of deep learning. Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form."]},{"cell_type":"markdown","metadata":{"id":"Bk5gV78Y1utI"},"source":["## The Linear Unit\n","\n","So let's begin with the fundamental component of a neural network: the individual neuron. As a diagram, a **neuron** (or **unit**) with one input looks like:\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/mfOlDR6.png\" width=\"250\" alt=\"Diagram of a linear unit.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>The Linear Unit: $y = w x + b$\n","</center></figcaption>\n","</figure>\n","\n","The input is `x`. Its connection to the neuron has a **weight** which is `w`. Whenever a value flows through a connection, you multiply the value by the connection's weight. For the input `x`, what reaches the neuron is `w * x`. A neural network \"learns\" by modifying its weights.\n","\n","The `b` is a special kind of weight we call the **bias**. The bias doesn't have any input data associated with it; instead, we put a `1` in the diagram so that the value that reaches the neuron is just `b` (since `1 * b = b`). The bias enables the neuron to modify the output independently of its inputs.\n","\n","The `y` is the value the neuron ultimately outputs. To get the output, the neuron sums up all the values it receives through its connections. This neuron's activation is `y = w * x + b`, or as a formula ùë¶ = ùë§ùë• + ùëè."]},{"cell_type":"markdown","metadata":{"id":"RVo1ukMkjg3n"},"source":["## Linear Units in Keras\n","\n","The easiest way to create a model in Keras is through `keras.Sequential`, which creates a neural network as a stack of *layers*.\n","\n","We could define a simple linear model accepting three input features and producing a single output like so:"]},{"cell_type":"code","metadata":{"id":"qtCJ0eGKchvI"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","# Create a network with 1 linear unit\n","model = keras.Sequential([\n","    layers.Dense(units=1, input_shape=[3])\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aUph8xIAsC1l"},"source":["## Layers\n","\n","Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/2MA4iMV.png\" width=\"300\" alt=\"A stack of three circles in an input layer connected to two circles in a dense layer.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>A dense layer of two linear units receiving two inputs and a bias.\n","</center></figcaption>\n","</figure>\n","\n","You could think of each layer in a neural network as performing some kind of relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways. In a well-trained neural network, each layer is a transformation getting us a little bit closer to a solution."]},{"cell_type":"markdown","metadata":{"id":"VNRzH5eEsmc7"},"source":["## The Activation Function\n","\n","It turns out, however, that two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something *nonlinear*. What we need are activation functions.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/OLSUEYT.png\" width=\"400\" alt=\" \">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Without activation functions, neural networks can only learn linear relationships. In order to fit curves, we'll need to use activation functions. \n","</center></figcaption>\n","</figure>\n","\n","An **activation function** is simply some function we apply to each of a layer's outputs (its *activations*). The most common is the *rectifier* function $max(0, x)$.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/aeIyAlF.png\" width=\"400\" alt=\"A graph of the rectifier function. The line y=x when x>0 and y=0 when x<0, making a 'hinge' shape like '_/'.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>\n","</center></figcaption>\n","</figure>\n","\n","The rectifier function has a graph that's a line with the negative part \"rectified\" to zero. Applying the function to the outputs of a neuron will put a *bend* in the data, moving us away from simple lines.\n","\n","When we attach the rectifier to a linear unit, we get a **rectified linear unit** or **ReLU**. (For this reason, it's common to call the rectifier function the \"ReLU function\".)  Applying a ReLU activation to a linear unit means the output becomes `max(0, w * x + b)`, which we might draw in a diagram like:\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/eFry7Yu.png\" width=\"250\" alt=\"Diagram of a single ReLU. Like a linear unit, but instead of a '+' symbol we now have a hinge '_/'. \">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>A rectified linear unit.\n","</center></figcaption>\n","</figure>\n","\n","The usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. (We'll see an example of this in Lesson 5 with *batch normalization*.) In this case, we can define the activation in its own `Activation` layer, like so:\n","\n","```\n","layers.Dense(units=8),\n","layers.Activation('relu')\n","```\n","\n","This is completely equivalent to the ordinary way: `layers.Dense(units=8, activation='relu')`.\n","\n","### Alternatives to ReLU\n","\n","There is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others -- all of which you can use in Keras. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with.\n","\n","Let's look at the graphs of some of these. Change the activation from `'relu'` to one of the others named above. Then run the cell to see the graph. (Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for more ideas.)"]},{"cell_type":"markdown","metadata":{"id":"29WCJQAos6nS"},"source":["## Stacking Dense Layers\n","\n","Now that we have some nonlinearity, let's see how we can stack layers to get complex data transformations.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/Y5iwFQZ.png\" width=\"450\" alt=\"An input layer, two hidden layers, and a final linear layer.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>A stack of dense layers makes a \"fully-connected\" network.\n","</center></figcaption>\n","</figure>\n","\n","The layers before the output layer are sometimes called **hidden** since we never see their outputs directly.\n","\n","Now, notice that the final (output) layer is a linear unit (meaning, no activation function). That makes this network appropriate to a regression task, where we are trying to predict some arbitrary numeric value. Other tasks (like classification) might require an activation function on the output.\n","\n","### Building Sequential Models ##\n","\n","The `Sequential` model we've been using will connect together a list of layers in order from first to last: the first layer gets the input, the last layer produces the output. This creates the model in the figure above:"]},{"cell_type":"code","metadata":{"id":"UbqnO-GGtGzo"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential([\n","    # the hidden ReLU layers\n","    layers.Dense(units=4, activation='relu', input_shape=[2]),\n","    layers.Dense(units=3, activation='relu'),\n","    # the linear output layer \n","    layers.Dense(units=1),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAutyLfutKUX"},"source":["Be sure to pass all the layers together in a list, like `[layer, layer, layer, ...]`, instead of as separate arguments. To add an activation function to a layer, just give its name in the `activation` argument."]},{"cell_type":"code","metadata":{"id":"16T4rL9unqBr"},"source":["model = keras.Sequential([\n","    layers.Dense(units=512, activation='relu', input_shape=[11]),\n","    layers.Dense(units=512, activation='relu'),\n","    layers.Dense(units=512, activation='relu'),\n","    layers.Dense(units=1)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gg3PVwlAt7_x"},"source":["## The Loss Function\n","\n","We've seen how to design an architecture for a network, but we haven't seen how to tell a network *what* problem to solve. This is the job of the loss function.\n","\n","The **loss function** measures the disparity between the the target's true value and the value the model predicts. \n","\n","Different problems call for different loss functions. We have been looking at **regression** problems, where the task is to predict some numerical value -- calories in *80 Cereals*, rating in *Red Wine Quality*. Other regression tasks might be predicting the price of a house or the fuel efficiency of a car.\n","\n","A common loss function for regression problems is the **mean absolute error** or **MAE**. For each prediction `y_pred`, MAE measures the disparity from the true target `y_true` by an absolute difference `abs(y_true - y_pred)`.\n","\n","The total MAE loss on a dataset is the mean of all these absolute differences.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/VDcvkZN.png\" width=\"500\" alt=\"A graph depicting error bars from data points to the fitted line..\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>The mean absolute error is the average length between the fitted curve and the data points.\n","</center></figcaption>\n","</figure>\n","\n","Besides MAE, other loss functions you might see for regression problems are the mean-squared error (MSE) or the Huber loss (both available in Keras).\n","\n","During training, the model will use the loss function as a guide for finding the correct values of its weights (lower loss is better). In other words, the loss function tells the network its objective.\n","\n","# The Optimizer - Stochastic Gradient Descent #\n","\n","We've described the problem we want the network to solve, but now we need to say *how* to solve it. This is the job of the **optimizer**. The optimizer is an algorithm that adjusts the weights to minimize the loss.\n","\n","Virtually all of the optimization algorithms used in deep learning belong to a family called **stochastic gradient descent**. They are iterative algorithms that train a network in steps. One **step** of training goes like this:\n","1. Sample some training data and run it through the network to make predictions.\n","2. Measure the loss between the predictions and the true values.\n","3. Finally, adjust the weights in a direction that makes the loss smaller.\n","\n","Then just do this over and over until the loss is as small as you like (or until it won't decrease any further.)\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/rFI1tIk.gif\" width=\"1600\" alt=\"Fitting a line batch by batch. The loss decreases and the weights approach their true values.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Training a neural network with Stochastic Gradient Descent.\n","</center></figcaption>\n","</figure>\n","\n","Each iteration's sample of training data is called a **minibatch** (or often just \"batch\"), while a complete round of the training data is called an **epoch**. The number of epochs you train for is how many times the network will see each training example.\n","\n","The animation shows the linear model from Lesson 1 being trained with SGD. The pale red dots depict the entire training set, while the solid red dots are the minibatches. Every time SGD sees a new minibatch, it will shift the weights (`w` the slope and `b` the y-intercept) toward their correct values on that batch. Batch after batch, the line eventually converges to its best fit. You can see that the loss gets smaller as the weights get closer to their true values.\n","\n","## Learning Rate and Batch Size ##\n","\n","Notice that the line only makes a small shift in the direction of each batch (instead of moving all the way). The size of these shifts is determined by the **learning rate**. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n","\n","The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious. (We'll explore these effects in the exercise.)\n","\n","Fortunately, for most work it won't be necessary to do an extensive hyperparameter search to get satisfactory results. **Adam** is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.\n","\n","## Adding the Loss and Optimizer ##\n","\n","After defining a model, you can add a loss function and optimizer with the model's `compile` method:\n","\n","```\n","model.compile(\n","    optimizer=\"adam\",\n","    loss=\"mae\",\n",")\n","```\n","\n","Notice that we are able to specify the loss and optimizer with just a string. You can also access these directly through the Keras API -- if you wanted to tune parameters, for instance -- but for us, the defaults will work fine."]},{"cell_type":"markdown","metadata":{"id":"oNnMlWGDwwUX"},"source":["The *Red Wine Quality* dataset consists of physiochemical measurements from about 1600 Portuguese red wines.  Also included is a quality rating for each wine from blind taste-tests. How well can we predict a wine's perceived quality from these measurements?\n","\n","First, run the next cell to display the first few rows of this dataset."]},{"cell_type":"code","metadata":{"id":"vEurNY20wu9L"},"source":["import pandas as pd\n","from IPython.display import display\n","\n","red_wine = pd.read_csv('red-wine.csv')\n","\n","# Create training and validation splits\n","df_train = red_wine.sample(frac=0.7, random_state=0)\n","df_valid = red_wine.drop(df_train.index) # validation set becomes dataframe - trainingset\n","display(df_train.head(4)) #displays the first few entries in the training set\n","\n","# Scale to [0, 1]\n","max_ = df_train.max(axis=0)\n","min_ = df_train.min(axis=0)\n","df_train = (df_train - min_) / (max_ - min_)\n","df_valid = (df_valid - min_) / (max_ - min_)\n","\n","# Split features and target\n","X_train = df_train.drop('quality', axis=1)\n","X_valid = df_valid.drop('quality', axis=1)\n","y_train = df_train['quality']\n","y_valid = df_valid['quality']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SC0GuhUyw7z7"},"source":["You can get the number of rows and columns of a dataframe (or a Numpy array) with the `shape` attribute.\n","\n","The `input_shape` parameter for a Keras model on this task should be equivalent to the number of features."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LAv52vOLw8c6","executionInfo":{"status":"ok","timestamp":1613528046190,"user_tz":300,"elapsed":274,"user":{"displayName":"Arnesh Sahay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gigokmr-b6HDELZvM8XBx_C52WACBzX8EBXsjQbeg=s64","userId":"11472057529334045403"}},"outputId":"c1b361c8-395b-4a98-c097-054ef5f8eb28"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1119, 11)"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"usaaGB7ewbyp"},"source":["Now we will define a linear model appropriate for this task. Pay attention to how many inputs and outputs the model should have. Eleven columns means eleven inputs.\n","\n","We've chosen a three-layer network with over 1500 neurons. This network should be capable of learning fairly complex relationships in the data."]},{"cell_type":"code","metadata":{"id":"bCowJzkLwc47"},"source":["model = keras.Sequential([\n","    layers.Dense(512, activation='relu', input_shape=[11]),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(512, activation='relu'),\n","    layers.Dense(1),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylNwj7Z-xOvg"},"source":["After defining the model, we compile in the optimizer and loss function."]},{"cell_type":"code","metadata":{"id":"IMvm7elWt6s_"},"source":["model.compile(\n","    optimizer=\"adam\",\n","    loss=\"mae\",\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6vd3CV5xWed"},"source":["Now we're ready to start training! We've told Keras to feed the optimizer 256 rows of the training data at a time (the `batch_size`) and to do that 10 times all the way through the dataset (the `epochs`)."]},{"cell_type":"code","metadata":{"id":"kvBxyOtEfdzf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613506674038,"user_tz":300,"elapsed":3012,"user":{"displayName":"Arnesh Sahay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gigokmr-b6HDELZvM8XBx_C52WACBzX8EBXsjQbeg=s64","userId":"11472057529334045403"}},"outputId":"0d3cc40c-372f-44ee-e2eb-63b7c73903a1"},"source":["history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_valid, y_valid),\n","    batch_size=256,\n","    epochs=10,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","5/5 [==============================] - 1s 130ms/step - loss: 0.3319 - val_loss: 0.1377\n","Epoch 2/10\n","5/5 [==============================] - 0s 31ms/step - loss: 0.1391 - val_loss: 0.1269\n","Epoch 3/10\n","5/5 [==============================] - 0s 32ms/step - loss: 0.1300 - val_loss: 0.1209\n","Epoch 4/10\n","5/5 [==============================] - 0s 32ms/step - loss: 0.1212 - val_loss: 0.1115\n","Epoch 5/10\n","5/5 [==============================] - 0s 33ms/step - loss: 0.1129 - val_loss: 0.1201\n","Epoch 6/10\n","5/5 [==============================] - 0s 33ms/step - loss: 0.1143 - val_loss: 0.1043\n","Epoch 7/10\n","5/5 [==============================] - 0s 31ms/step - loss: 0.1044 - val_loss: 0.1085\n","Epoch 8/10\n","5/5 [==============================] - 0s 38ms/step - loss: 0.1053 - val_loss: 0.1013\n","Epoch 9/10\n","5/5 [==============================] - 0s 32ms/step - loss: 0.1027 - val_loss: 0.1005\n","Epoch 10/10\n","5/5 [==============================] - 0s 32ms/step - loss: 0.1016 - val_loss: 0.1011\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oLTkq6irxemS"},"source":["You can see that Keras will keep you updated on the loss as the model trains.\n","\n","Often, a better way to view the loss though is to plot it. The `fit` method in fact keeps a record of the loss produced during training in a `History` object. We'll convert the data to a Pandas dataframe, which makes the plotting easy."]},{"cell_type":"code","metadata":{"id":"Oq9C_Z-affF8","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1613506680602,"user_tz":300,"elapsed":729,"user":{"displayName":"Arnesh Sahay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gigokmr-b6HDELZvM8XBx_C52WACBzX8EBXsjQbeg=s64","userId":"11472057529334045403"}},"outputId":"ab139b44-17ee-45ba-e2bf-f7ad378c7792"},"source":["# convert the training history to a dataframe\n","history_df = pd.DataFrame(history.history)\n","# use Pandas native plot method to display loss and validation loss through each epoch\n","history_df.loc[:,['loss','val_loss']].plot();"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"LY8o67GRxj8u"},"source":["Notice how the loss levels off as the epochs go by. When the loss curve becomes horizontal like that, it means the model has learned all it can and there would be no reason continue for additional epochs."]},{"cell_type":"markdown","metadata":{"id":"-J7Gc0syxwab"},"source":["## Overfitting and Underfitting\n","\n","There's more to the world of deep learning than just dense layers. There are dozens of kinds of layers you might add to a model. (Try browsing through the [Keras docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) for a sample!) Some are like dense layers and define connections between neurons, and others can do preprocessing or transformations of other sorts.\n","\n","We'll cover two kinds of special layers, not containing any neurons themselves, but that add some functionality that can sometimes benefit a model in various ways. Both are commonly used in modern architectures.\n","\n","## Dropout\n","\n","The first of these is the \"dropout layer\", which can help correct overfitting.\n","\n","In the last lesson we talked about how overfitting is caused by the network learning spurious patterns in the training data. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of \"conspiracy\" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.\n","\n","This is the idea behind **dropout**. To break up these conspiracies, we randomly *drop out* some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n","\n","<figure style=\"padding: 1em;\">\n","<img src=\"https://i.imgur.com/a86utxY.gif\" width=\"600\" alt=\"An animation of a network cycling through various random dropout configurations.\">\n","<figcaption style=\"textalign: center; font-style: italic\"><center>Here, 50% dropout has been added between the two hidden layers.</center></figcaption>\n","</figure>\n","\n","You could also think about dropout as creating a kind of *ensemble* of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)\n","\n","### Adding Dropout\n","\n","In Keras, the dropout rate argument `rate` defines what percentage of the input units to shut off. Put the `Dropout` layer just before the layer you want the dropout applied to:\n","\n","```\n","keras.Sequential([\n","    # ...\n","    layer.Dropout(rate=0.3), # apply 30% dropout to the next layer\n","    layer.Dense(16),\n","    # ...\n","])\n","```\n","\n","## Batch Normalization\n","\n","The next special layer we'll look at performs \"batch normalization\" (or \"batchnorm\"), which can help correct training that is slow or unstable.\n","\n","With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) or [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n","\n","Now, if it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the **batch normalization layer**. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n","\n","Most often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training.\n","\n","### Adding Batch Normalization\n","\n","It seems that batch normalization can be used at almost any point in a network. You can put it after a layer...\n","\n","```\n","layers.Dense(16, activation='relu'),\n","layers.BatchNormalization(),\n","```\n","\n","... or between a layer and its activation function:\n","\n","```\n","layers.Dense(16),\n","layers.BatchNormalization(),\n","layers.Activation('relu'),\n","```\n","\n","And if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn's `StandardScaler`.\n","\n","## Example - Using Dropout and Batch Normalization\n","\n","Let's continue developing the *Red Wine* model. Now we'll increase the capacity even more, but add dropout to control overfitting and batch normalization to speed up optimization. This time, we'll also leave off standardizing the data, to demonstrate how batch normalization can stabalize the training."]},{"cell_type":"markdown","metadata":{"id":"XjVmqWeQyYsJ"},"source":["Whan adding dropout, you may need to increase the number of units in your `Dense` layers."]},{"cell_type":"code","metadata":{"id":"P0HYnBLfyTOp"},"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","model = keras.Sequential([\n","    layers.Dense(1024, activation='relu', input_shape=[11]),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1024, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1024, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.BatchNormalization(),\n","    layers.Dense(1),\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TvcDOv8Fyegx"},"source":["There's nothing to change this time in how we set up the training."]},{"cell_type":"code","metadata":{"id":"OOXp1uU2yfUf"},"source":["model.compile(\n","    optimizer='adam',\n","    loss='mae',\n",")\n","\n","history = model.fit(\n","    X_train, y_train,\n","    validation_data=(X_valid, y_valid),\n","    batch_size=256,\n","    epochs=100,\n","    verbose=0,\n",")\n","\n","\n","# Show the learning curves\n","history_df = pd.DataFrame(history.history)\n","history_df.loc[:, ['loss', 'val_loss']].plot();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdwGpw0Qyk8I"},"source":["You'll typically get better performance if you standardize your data before using it for training. That we were able to use the raw data at all, however, shows how effective batch normalization can be on more difficult datasets."]}]}